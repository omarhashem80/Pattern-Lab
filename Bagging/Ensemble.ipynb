{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üë• Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In English, an `ensemble` is a group of musicians, actors, or dancers who regularly perform together. \n",
    "\n",
    "In machine learning, an ensemble model is a model $E$ that wraps a set of $T$ models ${M_1,M_2,...,M_T}$ where the prediction (i.e., classification or regression) of the model $E$ is decided by combining the predictions of the models ${M_1,M_2,...,M_T}$ in some way.\n",
    "\n",
    "#### There are four main types of Ensemble learning:\n",
    "\n",
    "\n",
    "- **üöÄ Boosting**\n",
    "\n",
    "- **üõçÔ∏è Bagging**\n",
    "\n",
    "\n",
    "- **üèõ Voting**\n",
    "\n",
    "- **ü•û Stacking**\n",
    "\n",
    "We will learn about each type below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen boosting in the last lab. In boosting, we sequentially train a set of models ${M_1,M_2,...,M_T}$ such that $M_{t}$ gives more emphasis to the errors of classifiers $M_{t-1}, M_{t-2},...$.\n",
    "\n",
    "AdaBoost is the original boosting algorithm and was developed by Freund and Schapire. After it was developed, it was observed that applying Adaboost is equivalent to iteratively minimizing the exponential loss function to find the weights of the classifiers.\n",
    "\n",
    "Thus, Gradient Boosting was developed as a generalization of AdaBoost that does not assume a particular loss function to minimize (and minimizes it via an optimization algorithm such as gradient descent). \n",
    "\n",
    "One other very well known and successful instance of gradient boosting is `Extreme Gradient Boosting` or `XGBoost` which mainly adds regularization to the algorithms (which we discussed before). With this, the decision trees can sequentially grow during boosting under the control of regularization. It also uses a faster optimization algorithm called Newton's method and introduces parallel processing. \n",
    "\n",
    "Other popular variants include `LightGBM`, `CatBoost` and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare Adaboost and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost            # It's not in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Generate the make_moons dataset\n",
    "x_data, y_data = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "n_estimators_list = [5, 10, 50, 100, 250, 500, 1000]\n",
    "adaboost_scores = []\n",
    "xgboost_scores = []\n",
    "\n",
    "for n_estimators in n_estimators_list:\n",
    "    # TODO 1: Train Adaboost with the given n_estimators, algorithm='SAMME', and random_state=42 then perform cross-validation with K=5\n",
    "    adaboost_clf = None\n",
    "    adaboost_cv_scores = None\n",
    "    adaboost_mean_cv_score = np.mean(adaboost_cv_scores)\n",
    "    adaboost_scores.append(adaboost_mean_cv_score)\n",
    "\n",
    "    # TODO 2: Train XGBoost with the given n_estimators and random_state=42 then perform cross-validation with K=5\n",
    "    xgboost_clf = None\n",
    "    xgboost_cv_scores = None\n",
    "    xgboost_mean_cv_score = np.mean(xgboost_cv_scores)\n",
    "    xgboost_scores.append(xgboost_mean_cv_score)\n",
    "\n",
    "# Plotting\n",
    "plt.style.use(\"dark_background\")\n",
    "plt.figure(figsize=(20, 5), dpi=100)  \n",
    "plt.plot(n_estimators_list, adaboost_scores, label='AdaBoost')\n",
    "plt.plot(n_estimators_list, xgboost_scores, label='XGBoost')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Mean Cross-Validation Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõçÔ∏è Bagging (Bootstrap Aggregation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bagging, after initializing the $T$ models $M_1, M_2,...,M_T$ we form $T$ bootstrapped training set $D_1, D_2,...,D_T$ by taking $T$ **random samples with replacement** from the original dataset $D$. The size of bootstrapped datasets is a ratio in $(0,1]$ of the size of $D$ that can be chosen as a hyperparameter.\n",
    "\n",
    " Afterwards, it trains model $M_i$ on dataset $D_i$ and then combines the predictions of the models $M_1, M_2,...,M_T$ by averaging for regression and majority voting or averaging probabilities (soft voting) for classification. \n",
    "\n",
    "In case of averaging probabilties, we have\n",
    "$$\\hat{y} = \\arg\\max_{c} \\left( \\frac{1}{T} \\sum_{i=1}^{T} p_{i,c} \\right)$$\n",
    "\n",
    "That is, we return the class with the highest average probability over all $T$ classifiers $M_1, M_2,...,M_T$. These models are different after training because they were trained on different (but overlapping) datasets $D_1, D_2,...,D_T$.\n",
    "\n",
    "In bagging, $M_i$ is usually a strong classifiers which may be even overfitting. It can be mathematically and visually shown that even if $M_i$ overfits $D_i$, their combination $E$ will reduce overfitting on the original training $D$. Thus, bagging can be sought when the model seems to be overfitting.\n",
    "\n",
    "<div align=\"center\">\n",
    "   <img width=\"900\" src=\"https://i.imgur.com/FLEa2bb.png\">\n",
    "</div>\n",
    "\n",
    "Utilizing strong classifiers is the first difference between bagging and boosting. The second different is that the classifiers are independently, and not sequentually trained.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "By your understanding of Random Forests, it follows from this definition that:\n",
    "\n",
    "$$\\text{Random Forest} = \\text{Bagging(Decision Tree)} + \\text{Column Subsampling}$$\n",
    "\n",
    "where column subsampling refers to using only $N_s$ features chosen randomly for each split in any tree.\n",
    "\n",
    "Thus, we will proceed as follows:\n",
    "1. Implement bagging for any model family $M$ with optional column subsampling in `Bagging.py`. \n",
    "\n",
    "      - We will use averaging probabilities instead of majority vote because it's faster.\n",
    "      \n",
    "2. Use it to implement Random Forest by letting the model family $M$ be decision tree in `RandomForest.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Bagging Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we had put these tests in the notebook for convenience; however, it's important to note that in industry what happens is:\n",
    "- A testing library such as `unittests` is used to wrap the tests in a class (each test given by a function that makes some `assertions`)\n",
    "\n",
    "- The file containing the tests (i.e., `BaggingTest.py`) is run via a Github action for each push, PR, etc. (simply by running the test file as in the cell below) and it's expected that this file must be extended when any new feature is added\n",
    "\n",
    "- Usually there are tools that check the coverage of the tests (i.e., what percentage of the lines of code in the implementation file(s) are processed in the test file). Usually, you should aim for close to `100%` coverage.\n",
    "\n",
    "- Large language models have made the testing process easier as it's not hard for them to generate the tests from described expected behaviour but this has to be done with extreme caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "command = \"python -m unittest discover -s . -p 'BaggingTest.py'\"\n",
    "try:\n",
    "    subprocess.run(command, shell=True, check=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error occurred while running tests:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the file only tests the two core additions of bagging: bootstrap sampling and majority voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üå≤üå≥ Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from RandomForest import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from Plot import plot_model_contours\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üß™ Quick End-to-end Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a weak test but we're constrained due to randomization and bagging is already tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = make_classification(n_samples=10000, n_classes=2, n_features=4, random_state=45)\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=10, max_features=2, min_samples_split=2, max_depth=45, random_state=42)\n",
    "random_forest_clf_sk = RFC(n_estimators=10, max_features=2, min_samples_split=2, max_depth=45, random_state=42)\n",
    "random_forest_clf.fit(x_data, y_data)\n",
    "random_forest_clf_sk.fit(x_data, y_data)\n",
    "assert np.allclose(random_forest_clf.score(x_data, y_data), random_forest_clf_sk.score(x_data, y_data),  atol=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reproduce what we did with decision trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "df = pd.read_csv('penguins-clean.csv')\n",
    "\n",
    "x_data_pd, y_data_pd = df.drop(columns=['Species']), df['Species']\n",
    "x_data, y_data = x_data_pd.to_numpy(), y_data_pd.to_numpy()\n",
    "y_data = LabelEncoder().fit_transform(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: Repeat the hyperparameter search done in DecisionTree.ipynb but this time for random forest\n",
    "# Must use their model for this as ours doesn't perfectly satisfy their API. After this we get back to using ours.\n",
    "clf = RFC()                                                                     \n",
    "\n",
    "# Define the hyperparameters distribution (use same ranges as before)\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(2, 100), \n",
    "    'min_samples_split': None,  \n",
    "    'min_samples_leaf': None,   \n",
    "    'max_depth': None,  \n",
    "    'min_impurity_decrease': None  \n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "clf_searched = None\n",
    "\n",
    "clf_searched.fit(x_data, y_data)\n",
    "\n",
    "relevant_columns = ['param_n_estimators', 'param_min_samples_split', 'param_min_samples_leaf', 'param_max_depth', 'param_min_impurity_decrease', \n",
    "                    'mean_test_score', 'std_test_score', 'rank_test_score']\n",
    "cv_results_df = pd.DataFrame(clf_searched.cv_results_)[relevant_columns].round(decimals=3).sort_values(by='rank_test_score')\n",
    "cv_results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "TODO 4: How much extra performance was perceived form using random forests?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Answer goes here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåÄ Decision Trees and Overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DecisionTreeClassifier model\n",
    "random_forest_clf = RandomForestClassifier(max_depth=15, max_samples=0.5, max_features=2, random_state=42)\n",
    "\n",
    "# TODO 5: Try 1, 5, 10, 20, 30, 70, 100 for n_estimators\n",
    "hyperparams_list = [\n",
    "    {'n_estimators': None},\n",
    "    {'n_estimators': None},\n",
    "    {'n_estimators': None},\n",
    "    {'n_estimators': None},\n",
    "    {'n_estimators': None},\n",
    "    {'n_estimators': None},\n",
    "    {'n_estimators': None},\n",
    "    {'n_estimators': None},\n",
    "]\n",
    "\n",
    "# Plot decision boundaries. You can go make some coffee by the time it finishes (or overclock your CPU).\n",
    "plot_model_contours(random_forest_clf, x_data, y_data, hyperparams_list=hyperparams_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "TODO 6: What effect did increasing the number of trees from 1 to 200 have on the decision boundary? \n",
    "Explain in light of overfitting and the ideal decision boundary.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Answer goes here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although we only apply bagging on decision trees in this experiment, `Bagging` can be used on any strong classifier (e.g., `SVM`, `QDA`, `NaiveBayes`, etc.) or regressor to reduce its ability to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèõ Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting is the simplest ensemble you can think of. It assumes that the models $M_1, M_2,...,M_T$ are heterogenous (e.g., one is an `SVM`, the other is `QDA` and so on) and it simply trains each model on the entire dataset then predicts labels by hard or soft        majority voting.\n",
    "\n",
    "\n",
    "Hard Voting:\n",
    "$$\\hat{y} = \\arg \\max_{c} \\sum_{i=1}^{T} I(y_i = c)$$ \n",
    "\n",
    "Soft Voting:\n",
    "$$\\hat{y} = \\arg\\max_{c} \\left( \\frac{1}{T} \\sum_{i=1}^{T} p_{i,c} \\right)$$\n",
    "\n",
    "In other words, it's similar to bagging except that know random sampling happens to the dataset or feature (i.e., each model trained on same original dataset) and the models are usually not of the same family.\n",
    "\n",
    "Like `Bagging`, it's easy to do `Voting` in Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "# 1. Load the breast cancer dataset\n",
    "x_data, y_data = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# 2. Define classifiers\n",
    "clf1 = LinearDiscriminantAnalysis()\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "classifiers = [('LDA', clf1), ('RF', clf2), ('GNB', clf3)]\n",
    "\n",
    "# 3. Train and evaluate each classifier individually using cross-validation\n",
    "for name, clf in classifiers:\n",
    "    scores = cross_val_score(clf, x_data, y_data, cv=5)  \n",
    "    print(f\"{name} Accuracy: {np.mean(scores):.4f}\")\n",
    "\n",
    "# 4. Hard voting classifier\n",
    "eclf1 = VotingClassifier(estimators=classifiers, voting='hard')\n",
    "\n",
    "# 5. Evaluate the hard voting classifier using cross-validation\n",
    "scores = cross_val_score(eclf1, x_data, y_data, cv=5)  \n",
    "print(f\"Hard Voting Classifier Accuracy: {np.mean(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this shows, the majority can be wrong sometimes (i.e., weaker models making similar mistakes and few strong models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•û Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking tries to improve upon voting by making the observation that it's not best to give the same weight to the different classifiers. It's smarter to train a `judge model` on top of these classifiers to learn which models are weak and which are strong by looking at their predictions and the true prediction and assign weights accordingly.\n",
    "\n",
    "Thus the voting schemes become,   \n",
    "$$\\hat{y} = \\arg \\max_{c} \\sum_{i=1}^{T} w_i*I(y_i = c)$$ \n",
    "\n",
    "Or\n",
    "$$\\hat{y} = \\arg\\max_{c} \\left( \\frac{1}{T} \\sum_{i=1}^{T} w_i*p_{i,c} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a great intuitive idea. In a more general sense, the judge can be any function (not necessarily a linear combination) of the classifiers. In other words, it can be any model in general and it will perceive the predicitons of the base models as its features and train to choose the correct label given their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load the breast cancer dataset\n",
    "x_data, y_data = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# 2. Define classifiers\n",
    "clf1 = LinearDiscriminantAnalysis()\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "base_classifiers = [('LDA', clf1), ('RF', clf2), ('GNB', clf3)]\n",
    "\n",
    "# 3.Define the stacking classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_classifiers,\n",
    "    final_estimator=RandomForestClassifier(n_estimators=50, random_state=1),\n",
    "    stack_method='predict'\n",
    ")\n",
    "         \n",
    "# 4.Evaluate the stacking classifier using cross-validation\n",
    "scores = cross_val_score(stacking_clf, x_data, y_data, cv=5) \n",
    "\n",
    "print(f\"Stacking Classifier Accuracy: {np.mean(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite commonly used in competitions ü§´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExeWd6OGhvc2lqcGtwajhpZHVxaWFnNGRxeXA4b2s3cXI3aG1xNXd6NiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/l49JHLpRSLhecYEmI/giphy.gif\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
