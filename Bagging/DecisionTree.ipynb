{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå≥ Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are one type of machine learning models that operate by deriving decision rules from the training set and using them for prediction purposes. Let $A$, $B$ and $C$ be some categorical features in the dataset and $W$ and $Z$ be some continuous features and let lowercase letters represent possible values for them. In this case, a general decision rule can takes the form:\n",
    "\n",
    "$$(A=a) ‚àß (B=b) ‚àß (C=c) ‚àß (W>w) ‚àß (Z>z) ‚üπ [y=k] $$\n",
    "\n",
    "Each path (and hence, leave) in a decision tree corresponds to a rule like this. \n",
    "\n",
    "$\\text{The training of the tree occurs as follow:}$\n",
    "\n",
    "- Choose a feature (e.g., $A$ or $W$) and split it into ($A=a_1$, $A=a_2$, ..., $A=a_n$ or $W>w, W ‚â§ W$ )\n",
    "   \n",
    "    - Splitting means to add as child nodes (it's splitting in the sense that the training data will be distributed over them)\n",
    "\n",
    "    - Multiple techniques were covered to choose the best feature\n",
    "\n",
    "- For each of the $n$ splits (i.e., children), repeat the process with the remaining features (e.g., $B$, $C$, $W$ and $Z$)\n",
    "\n",
    "- Stop when all instances are pure (i.e., all training points satisfying the conditions along the path from the root have the same label $y=k$) or when some other condition is met (e.g., to avoid overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Plot import plot_model_contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üòé  We are Wearing our Machine Learning Engineer Spectacles Early This Time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "TODO 1: Study the hyperparameters of the DecisionTreeClassifier as found in the documentation and answer the following:\n",
    "```\n",
    "1. Does increasing `min_samples_split` encourage a deeper or shallower tree and what effect does this have on overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Answer goes here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Answer the same question for increasing `min_impurity_decrease` and relate it to an equation in the lecture (try to write it here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Answer goes here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Devise an example where `min_samples_split=a` and `min_samples_leaf=b` and a split on a node is prevented although it has number of samples `c` where `c>a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Answer goes here. Can use https://tree.nathanfriend.io/ or draw it on paper or use a diagram tool.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that the decision tree will keep training (splitting nodes) until all leaves are pure, one of the three conditions above stops any further node from splitting or `max_depth` hyperparameter is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêß Load Penguins Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've done this many time in the past. So let's get over it quick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "df = pd.read_csv('penguins.csv')                    # has many features and three classes (Penguin type)\n",
    "\n",
    "# drop unwanted columns for the purposes of this experiment\n",
    "df = df.drop(['studyName', 'Sample Number', 'Date Egg', 'Comments', 'Island', 'Stage', 'Individual ID',\n",
    "              'Clutch Completion', 'Sex', 'Region'], axis=1)\n",
    "df = df.drop(['Culmen Depth (mm)', 'Flipper Length (mm)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)'], axis=1)\n",
    "\n",
    "# drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# save for easier access later\n",
    "df.to_csv('penguins-clean.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract `x_data` and `y_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_pd, y_data_pd = df.drop(columns=['Species']), df['Species']          \n",
    "x_data, y_data = x_data_pd.to_numpy(), y_data_pd.to_numpy()\n",
    "y_data = LabelEncoder().fit_transform(y_data)                               # convert labels to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëÄ Decision Trees in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by studying the performance of the decision tree on the Penguins dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: Define a decision tree classifier with max_depth=3 and random_state=42\n",
    "clf = None\n",
    "# TODO 3: Study the performance with 5-fold cross-validation (return the mean accuracy)\n",
    "acc = None\n",
    "round(acc, 3)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what did the decision tree have to go through to result in this. We will use the package `dtreeviz` to visualize it (which looks better and is more informative than the visualization provided by `Scikit-learn`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtreeviz         # install me with pip install dtreeviz\n",
    "\n",
    "# To simulate an arbitrary fold\n",
    "m = x_data.shape[0]\n",
    "x_train, y_train = x_data[:int(0.8*m),:], y_data[:int(0.8*m),]\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "viz = dtreeviz.model(clf, x_train, y_train, target_name='Penguin Type', feature_names=x_data_pd.columns, class_names=np.unique(y_data_pd))\n",
    "viz.view(scale=1.4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "TODO 4: What do you think makes splitting at 4175 for the second node in the second level better than any other value? \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Answer goes here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "TODO 5: How many decision rules where found by this decision tree? Which three rules are the strongest and why?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Answer goes here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as we have seen, the 90% accuracy is not so satisfactory. Let's dee if we can do better with hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîç Initialize Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6: Define a DecisionTree classifier with random_state=42\n",
    "clf = None\n",
    "\n",
    "# TODO 7: Define the hyperparameter search distribution\n",
    "param_dist = {\n",
    "    'min_samples_split': None,                      # 2 to 50\n",
    "    'min_samples_leaf': None,                       # 1 to 20\n",
    "    'max_depth': None,                              # 5 to 50 with step of 5\n",
    "    'min_impurity_decrease': None                   # Decide a reasonable range here (with 20 values)\n",
    "}\n",
    "\n",
    "# TODO 8: Initialize the Random Search object with 200 iterations, 5-fold cross-validation and random_state=42\n",
    "clf_searched = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîç Perform the Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the wrapped model (i.e., search and set the best hyperparameters)\n",
    "clf_searched.fit(x_data, y_data)\n",
    "\n",
    "### let's look at the top scores and the used hyperparameters:\n",
    "relevant_columns = ['param_min_samples_split', 'param_min_samples_leaf', 'param_max_depth', 'param_min_impurity_decrease', \n",
    "                    'mean_test_score', 'std_test_score', 'rank_test_score']\n",
    "cv_results_df = pd.DataFrame(clf_searched.cv_results_)[relevant_columns].round(decimals=3).sort_values(by='rank_test_score')\n",
    "# above we filtered the clf_searched.cv_results_ as it has other irrelevant columns then rounded all values in it and sorted by score\n",
    "\n",
    "cv_results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the best score is not that good. Let's plot to understand more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_contours(clf_searched,  x_data, y_data, trained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "TODO 9: Given the plot, comment on what disadvantage of decision trees, as covered in the lecture, is most likely the reason for the mediocre performance. Comparing the boundary to the ideal decision boundary shall be helpful in this.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer goes here.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåÄ Decision Trees and Overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DecisionTreeClassifier model\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# TODO 10: Use depth [1, 2, 3, 4, 6, 8, 15, 50]\n",
    "hyperparams_list = [\n",
    "    {'max_depth': None},\n",
    "    {'max_depth': None},\n",
    "    {'max_depth': None},\n",
    "    {'max_depth': None},\n",
    "    {'max_depth': None},\n",
    "    {'max_depth': None},\n",
    "    {'max_depth': None},\n",
    "    {'max_depth': None}\n",
    "]\n",
    "\n",
    "# Plot decision boundaries. \n",
    "plot_model_contours(dt_model, x_data, y_data, hyperparams_list=hyperparams_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "TODO 11: Given this, how does depth affect overfitting? Which depth seems the most optimal from the selection above?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer goes here.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  üåÄ Confirmatory Analysis for Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# TODO 12: Call make_moons with n_samples=1000, noise=0.3 and random_state=42\n",
    "x_data_m, y_data_m = None\n",
    "plot_model_contours(DecisionTreeClassifier(), x_data_m, y_data_m, hyperparams_list=hyperparams_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"https://i.imgur.com/LMiA2O5.gif\" width=800/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
