{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Adaboost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, we covered boosting, which is a technique that takes a week classifier (i.e., a model that severely underfits) and transforms it into a strong classifier by making many instances of such classifier and sequentially training them such that each subsequent one learns from the mistakes of the previous ones.\n",
    "\n",
    "An example of a week classifier is a decision stump. That is, a decision tree with one level. Such classifier is so week as it represents a constant hyperplane perpendicular to one of the axes (features).\n",
    "\n",
    "$$\n",
    "h_t(x) = \n",
    "\\begin{cases} \n",
    "1, & \\text{if } x_{\\text{k}} > C \\\\\n",
    "-1, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In other words, ordinary linear classifiers take the form $w_1 x_1 + w_2 x_2 + ... +w_n x_n + w_o = 0$. Meanwhile, this classifier takes the form $x_k + w_o=0$. It only learns $w_o$ and attempts to choose the best feature $x_k$ to which the hyperplane will pass through.\n",
    "\n",
    "### ðŸ’ª Training Algorithm\n",
    "\n",
    "**Initialize the weights:** Set initial weights $w_i = \\frac{1}{M}$ for each sample in the training set, where $M$ is the number of samples.\n",
    "\n",
    "**For each iteration $t$ from 1 to $T$:**\n",
    "1. **Train a weak classifier $h_t(x)$ using the training set weighted by $w_i$**\n",
    "\n",
    "$$h_t(x) \\leftarrow (x_1, x_2,...,x_m)$$\n",
    "\n",
    "2. **Compute the error:** Compute the weighted error $\\epsilon_t$ of the weak classifier:\n",
    "   $$\\epsilon_t = \\sum_{i=1}^{N} w_i \\cdot \\mathbb{I}(y_i \\neq h_t(x_i))$$\n",
    "   \n",
    "3. **Compute the weight $\\alpha_t$ of the weak classifier**:\n",
    "      $$\\alpha_t = \\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)$$\n",
    "   \n",
    "4. **Update the training weights:**:\n",
    "      $$w_i^{(t+1)} = w_i^{(t)} \\cdot \\exp(\\alpha_t \\mathbb{I}(y_i \\neq h_t(x_i))$$\n",
    "   \n",
    "\n",
    "5. **Normalize the Weights:**\n",
    "      $$w_i^{(t+1)} = \\frac {w_i^{(t)}}{\\sum_{i=1}^{M} w_i^{(t)}}$$\n",
    "\n",
    "### ðŸ”® Inference Algorithm\n",
    "\n",
    "3. **Output the final strong classifier:**\n",
    "      $$H(x) = \\text{argmax}_k(\\sum_{t=1}^{T} \\alpha_t \\mathbb{I}(h_t(x) = k))$$\n",
    "\n",
    "\n",
    "### ðŸ’Ž Special Notes\n",
    "\n",
    "- In general multiclass settings, inference takes place bu summing the $Î±$ of each group of classifier making the same vote for a specific class and then the class with the greatest sum is the final prediction as shown in the equation above. However, for your convenience we are implementing the binary classification version only here. **You should be able to easily show that** in this case if $y âˆˆ \\{+1, -1\\}$ the inference equation simplifies to:\n",
    "\n",
    " $$H(x) = \\text{sign}\\left(\\sum_{t=1}^{T} \\alpha_t h_t(x)\\right)$$\n",
    "\n",
    "- Notice that the term $\\mathbb{I}(y_i \\neq h_t(x_i))$ is used in both steps $(2)$ and $(4)$, we can compute it only once after training and use it in both expressions in the implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª End-to-end Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_blobs, make_circles, make_moons\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from Adaboost import Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "x_data, y_data = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=2, n_classes=2, random_state=42)\n",
    "y_data = np.where(y_data == 0, -1, 1)                                   # Convert labels to -1 and 1\n",
    "\n",
    "# Training and evaluating the custom AdaBoost\n",
    "T = 100\n",
    "\n",
    "# Training and evaluating custom and Scikit Adaboosts:\n",
    "adaboost_sklearn = AdaBoostClassifier(n_estimators=T, random_state=42, algorithm=\"SAMME\")\n",
    "adaboost_sklearn.fit(x_data, y_data)\n",
    "\n",
    "adaboost_custom = Adaboost(T=T, random_state=42)\n",
    "adaboost_custom.fit(x_data, y_data)\n",
    "\n",
    "# Assert the classifier weights (alphas) are set correctly\n",
    "assert np.allclose(adaboost_sklearn.estimator_weights_, adaboost_custom.Î±s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert they make the same prediction\n",
    "assert np.allclose(adaboost_custom.predict(x_data), adaboost_sklearn.predict(x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ˜Ž  Put on Your Machine Learning Engineer Spectacles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adaboost_contours(x_data, y_data, T_vals):\n",
    "    num_plots = len(T_vals)\n",
    "    num_cols = 4\n",
    "    # TODO 1: Compute the number of rows in the plot given num_cols and num_plots\n",
    "    num_rows = None\n",
    "    \n",
    "    # creating the meshgrid based on the min, max values for each of the two features in x_data\n",
    "    x1_min, x1_max = x_data[:, 0].min() - 0.1, x_data[:, 0].max() + 0.1\n",
    "    x2_min, x2_max = x_data[:, 1].min() - 0.1, x_data[:, 1].max() + 0.1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.1), np.arange(x2_min, x2_max, 0.1))\n",
    "    \n",
    "    plt.style.use('dark_background')\n",
    "    fig, ax = plt.subplots(num_rows, num_cols, figsize=(16, 4*num_rows), dpi=200)\n",
    "    \n",
    "    # for each value of T, generate a plot\n",
    "    for i, T in enumerate(T_vals):     \n",
    "        # TODO 2: Instantiate and fit your model with the current value of T and 42 for random_state   \n",
    "        adaboost = None\n",
    "        adaboost.fit(x_data, y_data)\n",
    "        Z = adaboost.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "        Z = Z.reshape(xx1.shape)\n",
    "        ax_curr = ax.flat[i]\n",
    "        ax_curr.contourf(xx1, xx2, Z, alpha=1.0, cmap='cividis')\n",
    "        ax_curr.scatter(x_data[:, 0], x_data[:, 1], c=y_data, s=20, edgecolor='k', cmap='plasma', linewidth=0.3)\n",
    "        ax_curr.set_title(f'T={T}')\n",
    "    \n",
    "    # clean up unused plots\n",
    "    for j in range(i+1, num_rows*num_cols):\n",
    "        fig.delaxes(ax.flat[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = make_circles(n_samples=1000, noise=0.2, factor=0.4, random_state=42)\n",
    "y_data = np.where(y_data == 0, -1, 1)                                   # Convert labels to -1 and 1\n",
    "# TODO 3: Pass 1, 2, 3, 5, 10, 50, 100, 500 for the T_vals\n",
    "T_vals = None\n",
    "plot_adaboost_contours(x_data, y_data, T_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's clear that the third classifier decided to draw it's decision boundary differently, explain why did it do that despite the fact the previous two classifiers did not. In other words, what has changed in the dataset to make draw such boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer goes here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why are the decision boundaries found by Adaboost in the instances above discontinuous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer goes here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the Number of Estimators T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = make_moons(n_samples=1000, noise=0.3, random_state=42)\n",
    "y_data = np.where(y_data == 0, -1, 1)                                   # Convert labels to -1 and 1\n",
    "# TODO 4: Pass 1, 2, 3, 5, 10, 50, 100, 200 for the T_vals\n",
    "T_vals = None\n",
    "plot_adaboost_contours(x_data, y_data, T_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you think is an optimal value for $T$ given these plots? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer goes here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given these plots how do you imagine the training loss curve? What technique would you use to automatically stop increasing T?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer goes here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = make_blobs(n_samples=1000, centers=2, n_features=2, cluster_std=2.5, random_state=42)\n",
    "y_data = np.where(y_data == 0, -1, 1)                                   # Convert labels to -1 and 1\n",
    "\n",
    "# TODO 5: Pass 1, 2, 3, 5, 10, 50, 100, 500 for the T_vals\n",
    "T_vals = None\n",
    "plot_adaboost_contours(x_data, y_data, T_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How do you imagine the decision boundary from LDA and is it better and worse than that due to Adaboost here and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer goes here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ¨ Extra Requirements List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make minor modifications to the function above so it itakes an arbitrary list of classifier objects and visualize the decision boundary of GNB, LDA, QDA and Adaboost.\n",
    "\n",
    "- Let the constructor take the config (object) of the base classifier and try out something different from the decision stump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"https://media1.tenor.com/m/eUBg0uKpgZIAAAAd/youve-done-an-amazing-job-lewis-jackson.gif\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
